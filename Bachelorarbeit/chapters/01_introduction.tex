% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}

% \section{Section}
The interactive theorem prover Isabelle~\parencite{Paulson1986} has been under active development for over 30 years. Over these years the project has grown considerably. The project is not only restricted to the distribution, but also includes the Archive of Formal Proofs, where many of the known Isabelle proofs can be found. This can be problematic, when trying to change something at the core of the system, because all peculiar characteristics of a functionality need to be emulated to offer a drop-in replacement. And yet the goal of this thesis is exactly that, with the aim to increase flexibility and dependability.

\section{Isabelle}
Isabelle is not only interactive, which means that proofs are authored within the program, while ideally getting immediate feedback, but also generic. It is generic with respect to the object logic. Other LCF descendants, like Nuprl, HOL or Coq, offer one single logic that can be used to formulate proofs in. Isabelle on the other hand, defines one basic logic, Pure, which is meant to host more involved logical systems, including FOL, ZF and HOL. Because of this it can be thought of as more of a framework, than a system for proofs by itself.

\subsection{Pure}
The following overview is rather shallow and might be supplemented by reading the relevant chapters in the Implementation Manual~\parencite{implementation}. Terms in Pure are represented by terms in the lambda calculus. The four important connectives are: \textit{universal quantification} ($\bigvee$), \textit{implication}($\Longrightarrow$), \textit{equality}($\equiv$) and \textit{conjunction}($\&\&\&$).\newline
A theorem is a structure that contains, among other things, a proposition. This proposition is a term that is assumed correct, under a list of meta hypotheses. On top of that, there is a background theory, that contains previously proofed lemmas. A theorem can be constructed using some primitive natural deduction inference rules and a few derived rules that operate on the internal of the theorems for performance reasons.\newline
The part of the Isabelle system that works on the internals of these theorems directly, or are depended on by these functions are considered the kernel. While these operations need to be checked very carefully for soundness, the soundness of the system as a whole, is merely a corollary of their soundness. Because of that, the rest of the system does not have to be put under such scrutiny.
\subsection{Resolution}
The rule almost exclusively used by the user-facing parts of the system, is the resolution rule. It facilitates using previously proofed theorems directly, without having to do the actual proof again. This is done by replacing one the premises of a theorem by all the premises of a second theorem.

\begin{figure}[ht]
\begin{displaymath}
    \prftree[r]{RS}
    {\left\llbracket A_x\,\middle|\, x \in \left[ m \right] \right\rrbracket \Longrightarrow B}
    {\left\llbracket B_x\,\middle|\, x \in \left[ n \right] \right\rrbracket \Longrightarrow C}
    {B\sigma \equiv B_i\sigma}
    {\left( \left\llbracket B_x\,\middle|\, x \in \left[ i - 1 \right] \right\rrbracket
    + \left\llbracket A_x\,\middle|\, x \in \left[ m \right] \right\rrbracket
    + \left\llbracket B_x\,\middle|\, x \in \left( \left[ n \right] \setminus \left[ i \right] \right) \right\rrbracket
    \Longrightarrow C \right) \sigma
    }
\end{displaymath}
\caption{Resolution}
\label{fig:res}
\end{figure}

For this replacement to be valid, the conclusion of the second theorem needs to be equivalent to the replaced premises of the first one. As this is not true in general, this equality is interpreted as a unification problem. Only if a unifier can be found, is the proposition obtained by applying this unifier, part of a new theorem. Unification of first-order terms is well understood and has nice properties, like a most general unifier. Pure, on the other hand, is not restricted to first-order terms, which is unfortunate from a implementation standpoint. Higher-order unification in general is not even decidable, but Huet~\parencite{Huet1975} has given a semi-decision procedure. It was more recently described by Dowek~\parencite{Dowek20011009} and is still the algorithm universally used.

\section{Goal}
For this thesis we investigate the possibility of pulling resolution out of the kernel by using existing primitives, where possible, to emulate the behavior of the current version outside of the kernel.\newline
Because the unifiers are not checked for correctness, they should make $B$ and $B_i$ syntactically equal, the whole unification procedure needs to be considered part of the kernel at the moment. A sensible version outside of the kernel, will have to verify the unifiers, with the gave benefit of removing the unification procedure from the kernel. This has not only the immediate benefit of a leaner kernel, but affords more flexibility for further development.\newline
It turns out that in many applications of Isabelle, higher order terms are actually not used. In these cases higher order unification can be harmful in different ways. While there is a performance penalty, the graver issue is the possibility of needlessly producing higher-order terms by applying the unifiers. Thus is would be desirable to give the user the choice of the unification algorithm. With the new resolution function, this would be a less substantial change, as this hypothetical first-order unification algorithm wouldn't have to be added to the kernel.\newline
The obvious drawback from abandoning direct manipulation of the theorems, is a degraded performance. Because resolution is used so intensively in virtually all Isabelle proofs, it's call billions of times when building the AFP, only a certain overhead is acceptable.\newline
That means, for an approach to be viable, it needs to balance reusing, possibly ill suited, primitives, with creating new specific ones. On the one extreme proof checking becomes unacceptably slow and the change will be rejected by the users and on the other side the new primitives add more code than they make obsolete as well as not being reusable.