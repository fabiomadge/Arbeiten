% !TeX root = ../main.tex
% Add the above to each chapter to make compiling the PDF easier in some editors.

\chapter{Introduction}\label{chapter:introduction}

% \section{Section}
The interactive theorem prover \textit{Isabelle}~\parencite{Paulson1986} has been under active development for over 30 years. Over this time span the project has grown considerably. When talking about the project, not only the distribution itself must be considered, but also the \textit{Archive of Formal Proofs}, where many of the known \textit{Isabelle} proofs can be found. This can cause headaches when trying to change something at the core of the system, because all peculiar characteristics of a given functionality need to be emulated to avoid problems on the outer layers of the system. And yet that is the goal set out for this thesis. The expected result is increased flexibility and dependability.

\section{Isabelle}
It is not only interactive, which means that proofs are authored within the program, while getting immediate feedback, but also generic. It is generic with respect to the object logic. Other LCF descendants, like \textit{Nuprl}, \textit{HOL4} or \textit{Coq}, offer one single logic that can be used to formulate proofs in. \textit{Isabelle} on the other hand, defines one basic meta-logic $\mathcal{M}$~\parencite{Paulson1989}, which is meant to host more involved logical systems, including first-order logic, Zermeloâ€“Fraenkel set theory and higher-order logic. Because of this it can be thought of as more of a framework for theorem proving, than a system for proofs by itself.

\subsection{Pure}
\textit{Isabelle/Pure} is the actual implementation of $\mathcal{M}$. The following overview is rather shallow and might be supplemented by reading the relevant chapters in the Implementation Manual~\parencite{implementation}. Terms in Pure are represented by terms in the lambda calculus. The three important connectives are: \textit{universal quantification} ($\bigvee$), \textit{implication}($\Longrightarrow$), \textit{equality}($\equiv$).\\
A \textit{theorem} is a structure that contains, among other things, a \textit{proposition}. This proposition is a term that is assumed correct, under a set of meta \textit{hypotheses}, which again are terms. On top of that, there is a background \textit{theory}, that contains previously proofed lemmas. A theorem can be constructed using some natural deduction inference rules and a few derived rules that operate on the internal of the theorems for performance reasons.\\
The part of the Isabelle system that works on the internals of theorems directly, or are depended on by these functions are considered the \textit{kernel}. These are the function that can introduce soundness or completeness deficiencies. While the kernel needs to be checked very carefully for correctness, the correctness of the system as a whole, is merely a corollary of it's correctness. Because of that, the rest of the system does not have to be put under such scrutiny.
\subsection{Resolution}
The rule almost exclusively used by the user-facing parts of the system to manipulate theorems, is the \textit{resolution} rule. It facilitates using previously proofed theorems directly, without having to do the actual proof again. This is done by replacing one the premises of a theorem by all the premises of a second theorem. The rule central to proving it's correctness is the \textit{modus ponens}.
\begin{figure}[ht]
\begin{displaymath}
    \prftree[r]{RS}
    {\left\llbracket A_x\,\middle|\, x \in \left[ m \right] \right\rrbracket \Longrightarrow B}
    {\left\llbracket B_x\,\middle|\, x \in \left[ n \right] \right\rrbracket \Longrightarrow C}
    {B\sigma \equiv B_i\sigma}
    {\left( \left\llbracket B_x\,\middle|\, x \in \left[ i - 1 \right] \right\rrbracket
    + \left\llbracket A_x\,\middle|\, x \in \left[ m \right] \right\rrbracket
    + \left\llbracket B_x\,\middle|\, x \in \left( \left[ n \right] \setminus \left[ i \right] \right) \right\rrbracket
    \Longrightarrow C \right) \sigma
    }
\end{displaymath}
\caption{Resolution}
\label{fig:res}
\end{figure}

For this replacement to be valid, the conclusion of the second theorem($B$) needs to be equivalent to the replaced premises($B_i$) of the first one. To ensure that this equality holds, it is interpreted as a unification problem. Only if a unifier can be found, a new theorem can be build, by applying the unifier to the resulting proposition. Unification of first-order terms is well understood and has nice properties, like a most general unifier. Pure, on the other hand, is not restricted to first-order terms. This is unfortunate from a implementation standpoint, but allows for more expressive object-logic's. Higher-order unification is not even decidable in general, but Huet~\parencite{Huet1975} has found a semi-decision procedure. It was more recently described by Dowek~\parencite{Dowek20011009} and is still the algorithm universally used.

\section{Goal}
For this thesis we investigate the possibility of pulling resolution out of the kernel by using existing primitives, where possible, to emulate the behavior of the current version outside of the kernel.\newline
Because the unifiers are not checked for correctness, they should make $B$ and $B_i$ syntactically equal, the whole unification procedure has to be considered part of the kernel at the moment. A sensible version outside of the kernel, will have to verify the unifiers, with the benefit of removing the unification procedure from the kernel. This has not only the immediate benefit of a leaner kernel, but affords more flexibility for further development.\newline
It turns out that in many applications of Isabelle, higher order terms are actually not used. In these cases higher order unification can be harmful in different ways. While there is a performance penalty, the graver issue is the possibility of needlessly producing higher-order terms by applying the unifiers. Thus is would be desirable to give the user the choice of the unification algorithm. With a new resolution function, this would be a less substantial change, as this hypothetical first-order unification algorithm wouldn't have to be added to the kernel.\newline
The obvious drawback from abandoning direct manipulation of the theorems, is a degraded performance. Because resolution is used so intensively in virtually all Isabelle proofs, it's called billions of times when building the AFP, only a certain overhead is acceptable.\\
That means, for an approach to be viable, it needs to balance reusing, possibly ill suited, primitives, with creating new specific ones. Both possible extreme have unacceptable consequences, in one the proof checking becomes unacceptably slow and the change will be rejected by the users. The other one adds some super specific not so primitives, whose combined footprint is bigger than what they replace.